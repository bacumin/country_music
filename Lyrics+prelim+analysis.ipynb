{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Connection object at 0x000001D2A7EE3D50>\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('C:/JupData/mxm_dataset.db')\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlite3.Cursor object at 0x000001D2A81EECE0>\n"
     ]
    }
   ],
   "source": [
    "res = conn.execute(\"SELECT * FROM sqlite_master WHERE type='table'\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_tmdb = sqlite3.connect('C:/JupData/track_metadata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('table', 'words', 'words', 2, 'CREATE TABLE words (word TEXT PRIMARY KEY)'),\n",
       " ('table',\n",
       "  'lyrics',\n",
       "  'lyrics',\n",
       "  4,\n",
       "  'CREATE TABLE lyrics (track_id, mxm_tid INT, word TEXT, count INT, is_test INT, FOREIGN KEY(word) REFERENCES words(word))')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conn.execute(\"SELECT word FROM words\")\n",
    "len(res.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('out',),\n",
       " ('down',),\n",
       " ('get',),\n",
       " ('she',),\n",
       " ('was',),\n",
       " ('see',),\n",
       " ('if',),\n",
       " ('got',),\n",
       " ('never',),\n",
       " ('from',),\n",
       " ('he',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conn.execute(\"SELECT word FROM words WHERE ROWID BETWEEN 50 AND 60\")\n",
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = conn.execute(\"SELECT track_id FROM lyrics WHERE word='countdown' ORDER BY RANDOM() LIMIT 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Old Crow Medicine Show', 'My Good Gal')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = conn_tmdb.execute(\"SELECT artist_name, title FROM songs WHERE track_id='TRCKLDK12903CEA5A9'\")\n",
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Got sick of SQL and switched to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Import data - select quantity with the number after \"limit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import words data\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_sql_query(\"select * from lyrics limit 10000000;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 5)\n",
      "                   track_id  mxm_tid   word  count  is_test\n",
      "9999995  TRPJVET128F92DE5CE  4145998  gotta      8        0\n",
      "9999996  TRPJVET128F92DE5CE  4145998   same      2        0\n",
      "9999997  TRPJVET128F92DE5CE  4145998   blue      1        0\n",
      "9999998  TRPJVET128F92DE5CE  4145998   real      2        0\n",
      "9999999  TRPJVET128F92DE5CE  4145998  chanc      2        0\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.tail())\n",
    "print(len(df['word'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import track information data to maps track id to title\n",
    "\n",
    "df_meta=pd.read_sql_query(\"SELECT * from songs limit 10000000;\", conn_tmdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 14)\n",
      "                  track_id                                title  \\\n",
      "999999  TRYYYVU12903CD01E3  Fernweh feat. Sektion Kuchik√§schtli   \n",
      "\n",
      "                   song_id     release           artist_id  \\\n",
      "999999  SOWXJXQ12AB0189F43  So Oder So  AR7PLM21187B990D08   \n",
      "\n",
      "                                 artist_mbid artist_name   duration  \\\n",
      "999999  3af2b07e-c91c-4160-9bda-f0b9e3144ed3       Texta  295.07873   \n",
      "\n",
      "        artist_familiarity  artist_hotttnesss  year  track_7digitalid  \\\n",
      "999999            0.552977           0.454869  2004           8486723   \n",
      "\n",
      "        shs_perf  shs_work  \n",
      "999999        -1         0  \n"
     ]
    }
   ],
   "source": [
    "print(df_meta.shape)\n",
    "print(df_meta.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_titles=df_meta.merge(df, on='track_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df_titles.shape, '\\n', df_titles[df_titles['track_id']=='TRAAPKW128F428BC93'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_titles.drop(labels=['song_id', 'shs_perf', 'shs_work', 'mxm_tid'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> first genres dataset, didn't end up using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first genres dataset\n",
    "\n",
    "df_genres=pd.read_csv('C:\\JupData\\msd-topMAGD-genreAssignment.cls', delimiter=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             track_id     genre\n",
      "0  TRAAAAK128F9318786  Pop_Rock\n"
     ]
    }
   ],
   "source": [
    "df_genres.columns=['track_id','genre']\n",
    "print(df_genres.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  track_id    genre\n",
      "28      TRAABHC128F933A3F8  Country\n",
      "93      TRAADXK12903CFAE92  Country\n",
      "144     TRAAGIU128F931BA5B  Country\n",
      "148     TRAAGMC128F4292D0F  Country\n",
      "150     TRAAGNL128F4299BF1  Country\n",
      "208     TRAAIAE128F42AC53D  Country\n",
      "323     TRAAMKI128F428537A  Country\n",
      "326     TRAAMRE128F4298E7D  Country\n",
      "334     TRAAMXN128F148AD52  Country\n",
      "343     TRAANEK128F427D2D5  Country\n",
      "363     TRAANTY12903CAE21E  Country\n",
      "372     TRAAOEH128F426A82F  Country\n",
      "374     TRAAOFQ128F92E1E46  Country\n",
      "391     TRAAORB128F930A97F  Country\n",
      "394     TRAAORZ128F421CB5C  Country\n",
      "407     TRAAPKW128F428BC93  Country\n",
      "415     TRAAPVB12903CDD85D  Country\n",
      "457     TRAARRM128F42BC439  Country\n",
      "604     TRAAXTL128F4222D39  Country\n",
      "624     TRAAYRA128F92E9CC7  Country\n",
      "720     TRABCLV128F428C59F  Country\n",
      "726     TRABCSP128F93089E8  Country\n",
      "758     TRABEDC128F421AFC7  Country\n",
      "905     TRABKKZ128F427D470  Country\n",
      "943     TRABLSO128F14A4707  Country\n",
      "997     TRABNTM128F42A2713  Country\n",
      "1011    TRABOGP12903CC080D  Country\n",
      "1033    TRABPDM128EF34E32D  Country\n",
      "1047    TRABPRT128F1495762  Country\n",
      "1138    TRABUAQ128F4262390  Country\n",
      "...                    ...      ...\n",
      "405370  TRZYHQC12903CB9603  Country\n",
      "405380  TRZYHXA128F14A3A4F  Country\n",
      "405388  TRZYIFL128F9315B2B  Country\n",
      "405413  TRZYJMY12903CC08FE  Country\n",
      "405523  TRZYOJU12903D01F36  Country\n",
      "405588  TRZYQRH128F42BC2B3  Country\n",
      "405674  TRZYUDY128F148AD53  Country\n",
      "405686  TRZYUMM12903CCE736  Country\n",
      "405714  TRZYVJB128F4264FDA  Country\n",
      "405751  TRZYXEU128F92FDC23  Country\n",
      "405764  TRZYXRH128F92FBF69  Country\n",
      "405930  TRZZEMC12903C9AB3F  Country\n",
      "405956  TRZZFWF128F4249BCA  Country\n",
      "405968  TRZZGGV128F932766C  Country\n",
      "405998  TRZZHRT128F93390B8  Country\n",
      "405999  TRZZHSJ128F4243A56  Country\n",
      "406030  TRZZJBT12903CC4921  Country\n",
      "406041  TRZZJNN128F92D7393  Country\n",
      "406093  TRZZLRG128F92FE8F2  Country\n",
      "406137  TRZZNIP12903CFDBBD  Country\n",
      "406185  TRZZPIZ128F148ADF2  Country\n",
      "406187  TRZZPKD128F428323F  Country\n",
      "406188  TRZZPKF128F92ED009  Country\n",
      "406214  TRZZQQO12903CFC9D6  Country\n",
      "406289  TRZZTYS128EF347EAB  Country\n",
      "406296  TRZZUFF128F427D2D6  Country\n",
      "406299  TRZZUIZ128F92FF374  Country\n",
      "406314  TRZZUYD128E0794EE6  Country\n",
      "406349  TRZZWXV128F428F20D  Country\n",
      "406376  TRZZYJR128F4272825  Country\n",
      "\n",
      "[11772 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_genres[df_genres['genre']=='Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> second genre dataset, nicer (more merges) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_genres2=pd.read_csv('C:\\JupData\\msd_tagtraum_cd1.cls', delimiter=\"\\t\", skiprows=8, usecols=[0,1], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             track_id       genre\n",
      "0  TRAAAAW128F429D538         Rap\n",
      "1  TRAAABD128F429CF47    Pop_Rock\n",
      "2  TRAAAED128E0783FAB        Jazz\n",
      "3  TRAAAEF128F4273421    Pop_Rock\n",
      "4  TRAAAEM128F93347B9  Electronic\n"
     ]
    }
   ],
   "source": [
    "df_genres2.columns=['track_id', 'genre']\n",
    "print(df_genres2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> merging genres data with titles and words data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged=df_titles.merge(df_genres2, on='track_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2795896, 15)\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             track_id           title         release           artist_id  \\\n",
      "0  TRMMMNO128F93539AA  In The Journey  In The Journey  AR4TLW81187B99683D   \n",
      "\n",
      "                            artist_mbid    artist_name   duration  \\\n",
      "0  0685ac4a-5cfc-408a-b391-903ea20e00bf  Martin Sexton  319.81669   \n",
      "\n",
      "   artist_familiarity  artist_hotttnesss  year  track_7digitalid word  count  \\\n",
      "0            0.641198           0.448653  2001           5749967    i     30   \n",
      "\n",
      "   is_test genre  \n",
      "0        0  Folk  \n"
     ]
    }
   ],
   "source": [
    "print(df_merged.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking that we have enough country data, making sure the data looks ok\n",
    "\n",
    "import numpy as np\n",
    "df_merged['is_country'] = np.where(df_merged['genre'] ==  'Country', 1,0)\n",
    "\n",
    "#df_merged['is_country']=0\n",
    "#df_merged['is_country'][df_merged['genre']=='Country']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop_Rock         1961352\n",
      "Rap               258306\n",
      "Country           143398\n",
      "RnB               117861\n",
      "Electronic         84984\n",
      "Folk               67849\n",
      "Reggae             47652\n",
      "Latin              43231\n",
      "Jazz               33939\n",
      "Blues              22210\n",
      "International       7737\n",
      "New Age             3750\n",
      "Vocal               3627\n",
      "Name: genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.genre.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   track_id           title        release  \\\n",
      "669      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "670      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "671      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "672      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "673      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "674      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "675      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "676      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "677      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "678      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "679      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "680      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "681      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "682      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "683      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "684      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "685      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "686      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "687      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "688      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "689      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "690      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "691      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "692      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "693      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "694      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "695      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "696      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "697      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "698      TRMMHIC128F92E8AE7         Rowboat      Unchained   \n",
      "...                     ...             ...            ...   \n",
      "2793965  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793966  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793967  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793968  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793969  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793970  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793971  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793972  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793973  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793974  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793975  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793976  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793977  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793978  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793979  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793980  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793981  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793982  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793983  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793984  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793985  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793986  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793987  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793988  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793989  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793990  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793991  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793992  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793993  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "2793994  TRKYVES128F14992EA  How Lucky I Am  Emerson Drive   \n",
      "\n",
      "                  artist_id                           artist_mbid  \\\n",
      "669      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "670      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "671      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "672      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "673      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "674      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "675      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "676      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "677      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "678      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "679      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "680      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "681      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "682      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "683      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "684      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "685      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "686      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "687      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "688      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "689      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "690      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "691      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "692      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "693      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "694      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "695      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "696      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "697      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "698      ARH861H1187B9B799E  d43d12a1-2dc9-4257-a2fd-0a3bb1081b86   \n",
      "...                     ...                                   ...   \n",
      "2793965  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793966  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793967  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793968  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793969  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793970  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793971  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793972  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793973  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793974  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793975  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793976  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793977  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793978  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793979  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793980  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793981  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793982  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793983  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793984  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793985  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793986  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793987  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793988  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793989  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793990  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793991  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793992  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793993  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "2793994  AR83QRD1187FB5BF49  5eba17b9-f67a-48e0-abc3-b9b0e98588b5   \n",
      "\n",
      "           artist_name   duration  artist_familiarity  artist_hotttnesss  \\\n",
      "669        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "670        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "671        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "672        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "673        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "674        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "675        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "676        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "677        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "678        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "679        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "680        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "681        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "682        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "683        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "684        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "685        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "686        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "687        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "688        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "689        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "690        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "691        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "692        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "693        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "694        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "695        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "696        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "697        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "698        Johnny Cash  223.94730            0.842333           0.747251   \n",
      "...                ...        ...                 ...                ...   \n",
      "2793965  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793966  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793967  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793968  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793969  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793970  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793971  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793972  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793973  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793974  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793975  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793976  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793977  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793978  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793979  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793980  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793981  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793982  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793983  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793984  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793985  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793986  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793987  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793988  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793989  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793990  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793991  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793992  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793993  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "2793994  Emerson Drive  233.40363            0.711970           0.467941   \n",
      "\n",
      "         year  track_7digitalid     word  count  is_test    genre  is_country  \n",
      "669      1996           5304803        i      3        0  Country           1  \n",
      "670      1996           5304803      the      8        0  Country           1  \n",
      "671      1996           5304803      you      4        0  Country           1  \n",
      "672      1996           5304803       to      4        0  Country           1  \n",
      "673      1996           5304803      and      4        0  Country           1  \n",
      "674      1996           5304803        a      3        0  Country           1  \n",
      "675      1996           5304803       me      6        0  Country           1  \n",
      "676      1996           5304803      not      3        0  Country           1  \n",
      "677      1996           5304803       in      4        0  Country           1  \n",
      "678      1996           5304803       my      6        0  Country           1  \n",
      "679      1996           5304803       is      5        0  Country           1  \n",
      "680      1996           5304803       of      3        0  Country           1  \n",
      "681      1996           5304803     your      2        0  Country           1  \n",
      "682      1996           5304803       do      3        0  Country           1  \n",
      "683      1996           5304803       on      1        0  Country           1  \n",
      "684      1996           5304803     will      6        0  Country           1  \n",
      "685      1996           5304803      all      2        0  Country           1  \n",
      "686      1996           5304803       no      4        0  Country           1  \n",
      "687      1996           5304803       be      9        0  Country           1  \n",
      "688      1996           5304803     have      1        0  Country           1  \n",
      "689      1996           5304803     this      1        0  Country           1  \n",
      "690      1996           5304803     with      2        0  Country           1  \n",
      "691      1996           5304803     like      1        0  Country           1  \n",
      "692      1996           5304803       up      2        0  Country           1  \n",
      "693      1996           5304803      out      1        0  Country           1  \n",
      "694      1996           5304803      she      8        0  Country           1  \n",
      "695      1996           5304803     away      3        0  Country           1  \n",
      "696      1996           5304803     more      3        0  Country           1  \n",
      "697      1996           5304803     been      1        0  Country           1  \n",
      "698      1996           5304803     give      2        0  Country           1  \n",
      "...       ...               ...      ...    ...      ...      ...         ...  \n",
      "2793965  2007            577201     soul      4        0  Country           1  \n",
      "2793966  2007            577201      new      3        0  Country           1  \n",
      "2793967  2007            577201     than      1        0  Country           1  \n",
      "2793968  2007            577201      sky      1        0  Country           1  \n",
      "2793969  2007            577201     blue      1        0  Country           1  \n",
      "2793970  2007            577201     danc      1        0  Country           1  \n",
      "2793971  2007            577201     tear      1        0  Country           1  \n",
      "2793972  2007            577201     rain      1        0  Country           1  \n",
      "2793973  2007            577201     star      1        0  Country           1  \n",
      "2793974  2007            577201     high      1        0  Country           1  \n",
      "2793975  2007            577201    smile      1        0  Country           1  \n",
      "2793976  2007            577201   togeth      1        0  Country           1  \n",
      "2793977  2007            577201    touch      3        0  Country           1  \n",
      "2793978  2007            577201  sometim      2        0  Country           1  \n",
      "2793979  2007            577201     road      2        0  Country           1  \n",
      "2793980  2007            577201   reason      1        0  Country           1  \n",
      "2793981  2007            577201     rise      1        0  Country           1  \n",
      "2793982  2007            577201     fill      4        0  Country           1  \n",
      "2793983  2007            577201   answer      2        0  Country           1  \n",
      "2793984  2007            577201      low      1        0  Country           1  \n",
      "2793985  2007            577201   window      4        0  Country           1  \n",
      "2793986  2007            577201     warm      4        0  Country           1  \n",
      "2793987  2007            577201    doubt      1        0  Country           1  \n",
      "2793988  2007            577201     soft      4        0  Country           1  \n",
      "2793989  2007            577201     glad      1        0  Country           1  \n",
      "2793990  2007            577201  endless      1        0  Country           1  \n",
      "2793991  2007            577201     okay      1        0  Country           1  \n",
      "2793992  2007            577201   prayer      1        0  Country           1  \n",
      "2793993  2007            577201    choic      1        0  Country           1  \n",
      "2793994  2007            577201    lucki      4        0  Country           1  \n",
      "\n",
      "[143398 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[df_merged['is_country']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#excluding pop rock\n",
    "df_merged2=df_merged[df_merged.genre!='Pop_Rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model=df_merged[['track_id', 'is_country', 'word', 'count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrix representation - efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "track_sorted=sorted(df_model.track_id.unique())\n",
    "words_sorted=sorted(df_model.word.unique())\n",
    "\n",
    "track_id_u = list(track_sorted)\n",
    "word_u = list(words_sorted)\n",
    "\n",
    "data = df_model['count'].tolist()\n",
    "row = df_model.track_id.astype('category', categories=track_id_u).cat.codes\n",
    "col = df_model.word.astype('category', categories=word_u).cat.codes\n",
    "sparse_matrix = csr_matrix((data, (row, col)), shape=(len(track_id_u), len(word_u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=np.array(sparse_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33780, 4998)\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[2860])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_train=df_model.groupby('track_id', sort=False)['is_country'].agg('mean')\n",
    "y_train=np.array(y_train)\n",
    "print(y_train[2980:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "model1=MultinomialNB()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.92      0.94     32194\n",
      "          1       0.19      0.38      0.25      1586\n",
      "\n",
      "avg / total       0.93      0.90      0.91     33780\n",
      "\n",
      "[[29655  2539]\n",
      " [  987   599]]\n"
     ]
    }
   ],
   "source": [
    "#Metrics\n",
    "expected=y_train\n",
    "predicted=model1.predict(x_train)\n",
    "print('training data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(y_train, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra data pre-processing for more expensive models\n",
    "SVM, Logistic and RF take a while to run, and could be prone to overfitting. The code below only keeps the top \"top\" most popular words (change 1 cell down to implement this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 1)\n"
     ]
    }
   ],
   "source": [
    "#Creates an array that sums up each column of x_train, sorts it, keeps the top 200 and subsets based on this\n",
    "x_train_pd=pd.DataFrame(x_train)\n",
    "x_train_sum=pd.DataFrame(x_train_pd.sum(axis=0))\n",
    "print(x_train_sum.shape)\n",
    "x_train_sum['idx']=range(len(x_train_sum))\n",
    "x_train_sum.sort_values(by=0, axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change top and the code below to exclude some observations.\n",
    "_from=0\n",
    "_to=500\n",
    "keep_columns=x_train_sum.iloc[_from:_to]['idx']\n",
    "keep_columns_array=np.array(keep_columns)\n",
    "x_train_subset=x_train_pd.iloc[:,keep_columns_array]\n",
    "x_train2=np.array(x_train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words (the stop words list is at the end of the notebook, for style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1868436, 4)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This significantly decreases our sample size :( hopefully it's all noise\n",
    "df_model_nostop=df_model.loc[~df_model.word.isin(stop_words)]\n",
    "df_model_nostop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "track_sorted=sorted(df_model_nostop.track_id.unique())\n",
    "words_sorted=sorted(df_model_nostop.word.unique())\n",
    "\n",
    "track_id_u = list(track_sorted)\n",
    "word_u = list(words_sorted)\n",
    "\n",
    "data = df_model_nostop['count'].tolist()\n",
    "row = df_model_nostop.track_id.astype('category', categories=track_id_u).cat.codes\n",
    "col = df_model_nostop.word.astype('category', categories=word_u).cat.codes\n",
    "sparse_matrix_nostop = csr_matrix((data, (row, col)), shape=(len(track_id_u), len(word_u)))\n",
    "\n",
    "x_train_nostop=np.array(sparse_matrix_nostop.todense())\n",
    "y_train_nostop=df_model_nostop.groupby('track_id', sort=False)['is_country'].agg('mean')\n",
    "y_train_nostop=np.array(y_train_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm afraid of overfitting without stop words. Create a subset of the top non stop words (note the index is shorter because of removal of stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4895, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_nostop_pd=pd.DataFrame(x_train_nostop)\n",
    "x_train_nostop_sum=pd.DataFrame(x_train_nostop_pd.sum(axis=0))\n",
    "print(x_train_nostop_sum.shape)\n",
    "x_train_nostop_sum['idx']=range(len(x_train_nostop_sum))\n",
    "x_train_nostop_sum.sort_values(by=0, axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change top and the code below to exclude some observations.\n",
    "_from=0\n",
    "_to=1000\n",
    "keep_columns=x_train_nostop_sum.iloc[_from:_to]['idx']\n",
    "keep_columns_array=np.array(keep_columns)\n",
    "x_train_nostop_subset=x_train_nostop_pd.iloc[:,keep_columns_array]\n",
    "x_train_nostop2=np.array(x_train_nostop_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33778, 1000) (33778,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_nostop2.shape,y_train_nostop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with only stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(927460, 4)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This significantly decreases our sample size :( hopefully it's all noise\n",
    "df_model_stop=df_model.loc[df_model.word.isin(stop_words)]\n",
    "df_model_stop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "track_sorted=sorted(df_model_stop.track_id.unique())\n",
    "words_sorted=sorted(df_model_stop.word.unique())\n",
    "\n",
    "track_id_u = list(track_sorted)\n",
    "word_u = list(words_sorted)\n",
    "\n",
    "data = df_model_stop['count'].tolist()\n",
    "row = df_model_stop.track_id.astype('category', categories=track_id_u).cat.codes\n",
    "col = df_model_stop.word.astype('category', categories=word_u).cat.codes\n",
    "sparse_matrix_stop = csr_matrix((data, (row, col)), shape=(len(track_id_u), len(word_u)))\n",
    "\n",
    "x_train_stop=np.array(sparse_matrix_stop.todense())\n",
    "y_train_stop=df_model_stop.groupby('track_id', sort=False)['is_country'].agg('mean')\n",
    "y_train_stop=np.array(y_train_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test/train split - choose data here. Options are:\n",
    "- x_train (all data, top 5k words)\n",
    "- x_train2 (all data, subset of words - see _from and _to 6 cells up)\n",
    "- x_train_nostop (no stop words)\n",
    "- x_train_nostop2 (no stop words, subset of words - see _from and _to 2 cells up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO CHANGE SAMPLE\n",
    "x=x_train\n",
    "y=y_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baptiste\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#regularization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(class_weight='balanced', C=1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2=logit.fit(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.97      0.88      2125\n",
      "          1       0.16      0.02      0.04       490\n",
      "\n",
      "avg / total       0.69      0.79      0.73      2615\n",
      "\n",
      "[[2060   65]\n",
      " [ 478   12]]\n",
      "training data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.99      0.90      5004\n",
      "          1       0.54      0.07      0.12      1096\n",
      "\n",
      "avg / total       0.78      0.82      0.76      6100\n",
      "\n",
      "[[4939   65]\n",
      " [1019   77]]\n"
     ]
    }
   ],
   "source": [
    "#Metrics test\n",
    "expected=Y_test\n",
    "predicted=model2.predict(X_test)\n",
    "print('test data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "#Metrics train\n",
    "expected=Y_train\n",
    "predicted=model2.predict(X_train)\n",
    "print('training data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_train, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(class_weight='balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3=rfc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.99      0.89      2125\n",
      "          1       0.10      0.00      0.01       490\n",
      "\n",
      "avg / total       0.68      0.81      0.73      2615\n",
      "\n",
      "[[2107   18]\n",
      " [ 488    2]]\n",
      "training data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      5004\n",
      "          1       0.99      0.86      0.92      1096\n",
      "\n",
      "avg / total       0.97      0.97      0.97      6100\n",
      "\n",
      "[[4998    6]\n",
      " [ 155  941]]\n"
     ]
    }
   ],
   "source": [
    "#Metrics test\n",
    "expected=Y_test\n",
    "predicted=model3.predict(X_test)\n",
    "print('test data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "#Metrics train\n",
    "expected=Y_train\n",
    "predicted=model3.predict(X_train)\n",
    "print('training data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_train, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svmlin=svm.LinearSVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4=svmlin.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.32      0.46      2125\n",
      "          1       0.19      0.67      0.29       490\n",
      "\n",
      "avg / total       0.69      0.39      0.43      2615\n",
      "\n",
      "[[ 690 1435]\n",
      " [ 163  327]]\n",
      "training data \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.36      0.51      5004\n",
      "          1       0.22      0.80      0.34      1096\n",
      "\n",
      "avg / total       0.77      0.44      0.48      6100\n",
      "\n",
      "[[1810 3194]\n",
      " [ 219  877]]\n"
     ]
    }
   ],
   "source": [
    "#Metrics test\n",
    "expected=Y_test\n",
    "predicted=model4.predict(X_test)\n",
    "print('test data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "#Metrics train\n",
    "expected=Y_train\n",
    "predicted=model4.predict(X_train)\n",
    "print('training data \\n' + metrics.classification_report(expected, predicted))\n",
    "matrix=metrics.confusion_matrix(Y_train, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Close sql connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()\n",
    "conn_tmdb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrix inefficient\n",
    "This is legacy code, it's ridiculously slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_model_piv=df_model.pivot(index='track_id', columns='word', values='count')\n",
    "print(df_model_piv.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_piv=df_model_piv.iloc[0:100000]\n",
    "df_model_piv.replace(to_replace='Nan', value=0, inplace=True)\n",
    "df_model_piv = df_model_piv.rename(columns={'word': 'track_id'})\n",
    "print(df_model_piv.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_is_country=df_merged[['track_id', 'is_country']]\n",
    "df_is_country=df_is_country.groupby(by='track_id', as_index=True).mean()\n",
    "print(df_is_country[df_is_country.is_country==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_sparse=pd.merge(df_model_piv, df_is_country, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_model_sparse.to_csv('C:/JupData/abc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "y_train=np.array(df_model_sparse['is_country'])\n",
    "df_model_sparse.drop('is_country', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_model_sparse.drop('track_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=np.array(df_model_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_train[2860], y_train[2980:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words=[\"a\",\n",
    "\"about\",\n",
    "\"above\",\n",
    "\"after\",\n",
    "\"again\",\n",
    "\"against\",\n",
    "\"all\",\n",
    "\"am\",\n",
    "\"an\",\n",
    "\"and\",\n",
    "\"any\",\n",
    "\"are\",\n",
    "\"aren't\",\n",
    "\"as\",\n",
    "\"at\",\n",
    "\"be\",\n",
    "\"because\",\n",
    "\"been\",\n",
    "\"before\",\n",
    "\"being\",\n",
    "\"below\",\n",
    "\"between\",\n",
    "\"both\",\n",
    "\"but\",\n",
    "\"by\",\n",
    "\"can't\",\n",
    "\"cannot\",\n",
    "\"could\",\n",
    "\"couldn't\",\n",
    "\"did\",\n",
    "\"didn't\",\n",
    "\"do\",\n",
    "\"does\",\n",
    "\"doesn't\",\n",
    "\"doing\",\n",
    "\"don't\",\n",
    "\"down\",\n",
    "\"during\",\n",
    "\"each\",\n",
    "\"few\",\n",
    "\"for\",\n",
    "\"from\",\n",
    "\"further\",\n",
    "\"had\",\n",
    "\"hadn't\",\n",
    "\"has\",\n",
    "\"hasn't\",\n",
    "\"have\",\n",
    "\"haven't\",\n",
    "\"having\",\n",
    "\"he\",\n",
    "\"he'd\",\n",
    "\"he'll\",\n",
    "\"he's\",\n",
    "\"her\",\n",
    "\"here\",\n",
    "\"here's\",\n",
    "\"hers\",\n",
    "\"herself\",\n",
    "\"him\",\n",
    "\"himself\",\n",
    "\"his\",\n",
    "\"how\",\n",
    "\"how's\",\n",
    "\"i\",\n",
    "\"i'd\",\n",
    "\"i'll\",\n",
    "\"i'm\",\n",
    "\"i've\",\n",
    "\"if\",\n",
    "\"in\",\n",
    "\"into\",\n",
    "\"is\",\n",
    "\"isn't\",\n",
    "\"it\",\n",
    "\"it's\",\n",
    "\"its\",\n",
    "\"itself\",\n",
    "\"let's\",\n",
    "\"me\",\n",
    "\"more\",\n",
    "\"most\",\n",
    "\"mustn't\",\n",
    "\"my\",\n",
    "\"myself\",\n",
    "\"no\",\n",
    "\"nor\",\n",
    "\"not\",\n",
    "\"of\",\n",
    "\"off\",\n",
    "\"on\",\n",
    "\"once\",\n",
    "\"only\",\n",
    "\"or\",\n",
    "\"other\",\n",
    "\"ought\",\n",
    "\"our\",\n",
    "\"ours\",\n",
    "\"ourselves\",\n",
    "\"out\",\n",
    "\"over\",\n",
    "\"own\",\n",
    "\"same\",\n",
    "\"shan't\",\n",
    "\"she\",\n",
    "\"she'd\",\n",
    "\"she'll\",\n",
    "\"she's\",\n",
    "\"should\",\n",
    "\"shouldn't\",\n",
    "\"so\",\n",
    "\"some\",\n",
    "\"such\",\n",
    "\"than\",\n",
    "\"that\",\n",
    "\"that's\",\n",
    "\"the\",\n",
    "\"their\",\n",
    "\"theirs\",\n",
    "\"them\",\n",
    "\"themselves\",\n",
    "\"then\",\n",
    "\"there\",\n",
    "\"there's\",\n",
    "\"these\",\n",
    "\"they\",\n",
    "\"they'd\",\n",
    "\"they'll\",\n",
    "\"they're\",\n",
    "\"they've\",\n",
    "\"this\",\n",
    "\"those\",\n",
    "\"through\",\n",
    "\"to\",\n",
    "\"too\",\n",
    "\"under\",\n",
    "\"until\",\n",
    "\"up\",\n",
    "\"very\",\n",
    "\"was\",\n",
    "\"wasn't\",\n",
    "\"we\",\n",
    "\"we'd\",\n",
    "\"we'll\",\n",
    "\"we're\",\n",
    "\"we've\",\n",
    "\"were\",\n",
    "\"weren't\",\n",
    "\"what\",\n",
    "\"what's\",\n",
    "\"when\",\n",
    "\"when's\",\n",
    "\"where\",\n",
    "\"where's\",\n",
    "\"which\",\n",
    "\"while\",\n",
    "\"who\",\n",
    "\"who's\",\n",
    "\"whom\",\n",
    "\"why\",\n",
    "\"why's\",\n",
    "\"with\",\n",
    "\"won't\",\n",
    "\"would\",\n",
    "\"wouldn't\",\n",
    "\"you\",\n",
    "\"you'd\",\n",
    "\"you'll\",\n",
    "\"you're\",\n",
    "\"you've\",\n",
    "\"your\",\n",
    "\"yours\",\n",
    "\"yourself\",\n",
    "\"yourselves\",\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
